name: Automated Assignment Grading

on:
  push:
    paths:
      - 'Submissions/**'
  pull_request:
    paths:
      - 'Submissions/**'
  workflow_dispatch:
    inputs:
      generate_feedback:
        description: 'Generate individual feedback files'
        required: false
        default: 'true'
        type: boolean

jobs:
  grade-assignments:
    runs-on: ubuntu-latest

    permissions:
      contents: write
      pull-requests: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install nbformat

    - name: Run assignment grading
      run: |
        python grade_assignments.py \
          --repo-path . \
          --output grading_results.json \
          $(if [ "${{ github.event.inputs.generate_feedback || 'false' }}" = "true" ]; then echo "--generate-feedback"; fi)

    - name: Upload grading results
      uses: actions/upload-artifact@v3
      with:
        name: grading-results
        path: |
          grading_results.json
          feedback/
          grader.log
        retention-days: 30

    - name: Create summary comment (for PRs)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const results = JSON.parse(fs.readFileSync('grading_results.json', 'utf8'));
            const summary = results._summary || {};

            const comment = `## ðŸ“Š Assignment Grading Summary

\\*\\*Grading Results:\\*\\*
- ðŸ‘¥ Students: ${summary.total_students || 0}
- ðŸ“ Submissions: ${summary.total_submissions || 0}
- ðŸ“ˆ Average Grade: ${summary.average_grade || 0}/100
- ðŸ¤– AI Detection Flags: ${summary.ai_flagged_submissions || 0} (${summary.ai_flagged_percentage || 0}%)

\\*Detailed results and feedback files are available in the workflow artifacts.\\*`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.error('Error creating comment:', error);
          }

    - name: Commit feedback files (if main branch)
      if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || github.event.inputs.generate_feedback == 'true')
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        if [ -d "feedback" ]; then
          git add feedback/
          git add grading_results.json

          if ! git diff --staged --quiet; then
            num_students=$(ls feedback/*.md 2>/dev/null | wc -l)
            avg_grade=$(python -c "import json; data=json.load(open('grading_results.json')); print(data.get('_summary', {}).get('average_grade', 'N/A'))")
            ai_flags=$(python -c "import json; data=json.load(open('grading_results.json')); print(data.get('_summary', {}).get('ai_flagged_submissions', 0))")

            git commit -m "ðŸ“Š Update assignment feedback and grading results

- Generated feedback for ${num_students} students
- Average grade: ${avg_grade}/100
- AI detection flags: ${ai_flags}

Auto-generated by assignment grader"

            git push
          else
            echo "No changes to commit"
          fi
        fi

    - name: Generate assignment report
      run: |
        python -c "
import json
import sys
from datetime import datetime

try:
    with open('grading_results.json', 'r') as f:
        results = json.load(f)

    summary = results.get('_summary', {})

    print('# Assignment Grading Report')
    print(f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    print()
    print('## Summary Statistics')
    print(f'- **Total Students:** {summary.get("total_students", 0)}')
    print(f'- **Total Submissions:** {summary.get("total_submissions", 0)}')
    print(f'- **Average Grade:** {summary.get("average_grade", 0)}/100')
    print(f'- **AI Detection Flags:** {summary.get("ai_flagged_submissions", 0)} ({summary.get("ai_flagged_percentage", 0)}%)')
    print()

    print('## Grade Distribution')
    grade_dist = summary.get('grade_distribution', {})
    for grade_range, count in grade_dist.items():
        print(f'- **{grade_range}:** {count} students')
    print()

    if summary.get('ai_flagged_submissions', 0) > 0:
        print('## âš ï¸ AI Detection Alerts')
        print('The following submissions were flagged for possible AI assistance:')
        print()

        for student_name, student_data in results.items():
            if student_name == '_summary':
                continue

            flagged_submissions = []
            for assignment_type in ['individual', 'group', 'project']:
                if assignment_type in student_data:
                    for submission in student_data[assignment_type].get('submissions', []):
                        ai_likelihood = submission['ai_detection'].get('likelihood', 'Very Low')
                        if ai_likelihood in ['High', 'Very High']:
                            flagged_submissions.append({
                                'file': submission['file_name'],
                                'type': assignment_type,
                                'likelihood': ai_likelihood,
                                'confidence': submission['ai_detection'].get('confidence', 0)
                            })

            if flagged_submissions:
                print(f'### {student_name}')
                for sub in flagged_submissions:
                    print(f'- **{sub["file"]}** ({sub["type"]}): {sub["likelihood"]} likelihood ({sub["confidence"]}% confidence)')
                print()

    print('## Recommendations')
    avg_grade = summary.get('average_grade', 0)
    if avg_grade >= 85:
        print('- Overall class performance is excellent!')
    elif avg_grade >= 75:
        print('- Good class performance overall. Some students may need additional support.')
    elif avg_grade >= 65:
        print('- Class performance is satisfactory but could be improved.')
    else:
        print('- Class performance needs significant improvement. Consider additional tutorials or office hours.')

    if summary.get('ai_flagged_submissions', 0) > 0:
        print('- Address AI assistance concerns with flagged students.')
        print('- Consider discussing academic integrity policies.')

except FileNotFoundError:
    print('Grading results file not found.')
    sys.exit(1)
except Exception as e:
    print(f'Error generating report: {e}')
    sys.exit(1)
" > GRADING_REPORT.md

    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: grading-report
        path: GRADING_REPORT.md
        retention-days: 90
